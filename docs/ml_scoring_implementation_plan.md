# ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹ãƒ»æ©Ÿæ¢°å­¦ç¿’ã‚’æ´»ç”¨ã—ãŸã‚¹ã‚³ã‚¢è¨ˆç®—ãƒ­ã‚¸ãƒƒã‚¯å®Ÿè£…è¨ˆç”»æ›¸

**ä½œæˆæ—¥**: 2026-02-08
**ä½œæˆè€…**: Claude Code (Sonnet 4.5)
**ãƒ¬ãƒ“ãƒ¥ãƒ¼å¯¾è±¡è€…**: ä»–ã®AIã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã€ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ãƒ†ã‚£ã‚¹ãƒˆ
**ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ**: Zoom UP Public App - Local Gov DX Intelligence

---

## ğŸ“‹ Executive Summaryï¼ˆã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–ã‚µãƒãƒªãƒ¼ï¼‰

### ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ç›®çš„
å…¨å›½1,741è‡ªæ²»ä½“ã®ã€ŒZoomå°å…¥ç¢ºåº¦ã€ã‚’æ©Ÿæ¢°å­¦ç¿’ã§äºˆæ¸¬ã—ã€å–¶æ¥­æ´»å‹•ã®åŠ¹ç‡ã‚’æœ€å¤§åŒ–ã™ã‚‹ã€‚

### å®Ÿè£…ã®ãƒ¢ãƒƒãƒˆãƒ¼
**"Garbage in, Garbage out"** - è³ªã®é«˜ã„ãƒ‡ãƒ¼ã‚¿åé›†ã¨åˆ†æã‚’å¾¹åº•ã—ã€ä¿¡é ¼ã§ãã‚‹äºˆæ¸¬ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã™ã‚‹ã€‚

### æŠ€è¡“ã‚¹ã‚¿ãƒƒã‚¯
- **Machine Learning**: Light GBM (ä¸»åŠ›ãƒ¢ãƒ‡ãƒ«)
- **Deep Learning**: BERTï¼ˆãƒ†ã‚­ã‚¹ãƒˆåˆ†æï¼‰ã€LSTMï¼ˆæ™‚ç³»åˆ—äºˆæ¸¬ï¼‰
- **Feature Engineering**: pandas, numpy, scikit-learn
- **Explainability**: SHAP
- **Infrastructure**: Lenovo Tiny (AI Engine) + AWS Lightsail (Frontend)
- **Notification**: Zoom Team Chat

### å®Ÿè£…è¦æ¨¡
- **ãƒ•ã‚§ãƒ¼ã‚º1-4ï¼ˆå¿…é ˆï¼‰**: ç´„1,000-1,500è¡Œã€æ‰€è¦æ™‚é–“ 6-8é€±é–“
- **ãƒ•ã‚§ãƒ¼ã‚º5ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰**: Deep Learningçµ±åˆã€è¿½åŠ 2-3é€±é–“

---

## ğŸ¯ Contextï¼ˆèƒŒæ™¯ï¼‰

### ç¾çŠ¶åˆ†æ

#### âœ… æ—¢ã«å®Ÿè£…ã•ã‚Œã¦ã„ã‚‹æ©Ÿèƒ½
1. **ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†ã‚·ã‚¹ãƒ†ãƒ **
   - Google Custom Search APIã«ã‚ˆã‚‹è‡ªæ²»ä½“ãƒ‰ãƒ¡ã‚¤ãƒ³ï¼ˆlg.jp, go.jpï¼‰é™å®šæ¤œç´¢
   - å®Ÿè£…å ´æ‰€: `backend/services/news_collector.py`

2. **LLMåˆ†ææ©Ÿèƒ½**
   - Ollama (Llama3) ã«ã‚ˆã‚‹ãƒ‹ãƒ¥ãƒ¼ã‚¹å€‹åˆ¥ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ï¼ˆ0-100ç‚¹ï¼‰
   - å®Ÿè£…å ´æ‰€: `backend/services/llm_analyzer.py`
   - ã‚¹ã‚³ã‚¢åŸºæº–:
     - 80-100: äºˆç®—æ‰¿èªã€å…¥æœ­å…¬å‘Šã€Zoomå°å…¥è¨€åŠ
     - 60-79: æ¤œè¨é–‹å§‹ã€ãƒ‘ã‚¤ãƒ­ãƒƒãƒˆã€DXæ¨é€²è¨ˆç”»ç­–å®š
     - 40-59: ä¸€èˆ¬çš„ãªDXãƒˆãƒ”ãƒƒã‚¯
     - 0-39: ç„¡é–¢ä¿‚

3. **ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ§‹é€ **
   - PostgreSQL (TimescaleDB) - æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿å¯¾å¿œ
   - 11ãƒ†ãƒ¼ãƒ–ãƒ«ï¼ˆmunicipalities, scores, news_statements, budgets, tendersç­‰ï¼‰
   - å®Ÿè£…å ´æ‰€: `backend/db/init.sql`

#### âŒ æœªå®Ÿè£…ã®æ©Ÿèƒ½ï¼ˆä»Šå›å®Ÿè£…ã™ã‚‹ï¼‰
1. **è‡ªæ²»ä½“å…¨ä½“ã®ã‚¹ã‚³ã‚¢è¨ˆç®—ãƒ­ã‚¸ãƒƒã‚¯**
   - ç¾çŠ¶: `municipalities.score_total`ã¯ä»®ãƒ‡ãƒ¼ã‚¿ã®ã¿
   - ç›®æ¨™: è¤‡æ•°ã®ã‚·ã‚°ãƒŠãƒ«ï¼ˆãƒ‹ãƒ¥ãƒ¼ã‚¹ã€äºˆç®—ã€å…¥æœ­ï¼‰ã‚’çµ±åˆã—ãŸã‚¹ã‚³ã‚¢

2. **ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°**
   - éæ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ï¼ˆãƒ†ã‚­ã‚¹ãƒˆï¼‰ã‹ã‚‰æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã¸ã®å¤‰æ›

3. **ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯æ©Ÿæ§‹**
   - "Garbage in, Garbage out"å¯¾ç­–

4. **æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ»æ¨è«–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³**

5. **Human-in-the-loopã«ã‚ˆã‚‹ç¶™ç¶šå­¦ç¿’**

### AIæˆ¦ç•¥ã¨ã®å¯¾å¿œï¼ˆdocs/ai_strategy.mdï¼‰

| ãƒ•ã‚§ãƒ¼ã‚º | å†…å®¹ | å®Ÿè£…çŠ¶æ³ |
|---------|------|---------|
| **Phase 1** | ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ç²—é¸åˆ¥ | âœ… å®Ÿè£…æ¸ˆã¿ |
| **Phase 2** | LLMã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚° | âœ… å®Ÿè£…æ¸ˆã¿ï¼ˆå€‹åˆ¥ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ã¿ï¼‰|
| **Phase 3** | æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã¸ã®å¤‰æ› | âš ï¸ **ä»Šå›å®Ÿè£…** |

### ã‚¤ãƒ³ãƒ•ãƒ©æ§‹æˆ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   AWS Lightsail (Tokyo Region)   â”‚
â”‚   - Next.js Frontend              â”‚
â”‚   - Cost: $10/month               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â†• Tailscale VPN
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Lenovo Tiny (Home, 24/7)       â”‚
â”‚   - Ollama (Llama3)               â”‚
â”‚   - PostgreSQL (TimescaleDB)      â”‚
â”‚   - Redis Cache                   â”‚
â”‚   - FastAPI (Full)                â”‚
â”‚   - Node-RED                      â”‚
â”‚   - Cost: é›»æ°—ä»£ ~$3/month         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ—ï¸ Architecture Designï¼ˆã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£è¨­è¨ˆï¼‰

### å¤šå±¤MLã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ 

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Layer 1: Data Collection & Quality Check             â”‚
â”‚  Raw Data â†’ Feature Engineering â†’ "Garbage Out" Filter       â”‚
â”‚                                                               â”‚
â”‚  - è‡ªæ²»ä½“Webã€äºˆç®—ã€å…¥æœ­ã€ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’åé›†                        â”‚
â”‚  - æ¬ æå€¤ãƒã‚§ãƒƒã‚¯ã€é®®åº¦ãƒã‚§ãƒƒã‚¯ã€ç•°å¸¸å€¤æ¤œå‡º                      â”‚
â”‚  - å“è³ªã‚¹ã‚³ã‚¢ < 0.7 ã®å ´åˆã¯ã‚¢ãƒ©ãƒ¼ãƒˆ                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Layer 2: Feature Store (æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿)                 â”‚
â”‚                                                               â”‚
â”‚  â‘  Tabular Featuresï¼ˆè¡¨å½¢å¼ç‰¹å¾´é‡ï¼‰                            â”‚
â”‚     - population_log, population_density                     â”‚
â”‚     - dx_budget_trend, budget_allocation_ratio               â”‚
â”‚     - tender_count_1y, has_zoom_related_tender               â”‚
â”‚                                                               â”‚
â”‚  â‘¡ Text Featuresï¼ˆãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´é‡ï¼‰                             â”‚
â”‚     - sentiment_score_avg, positive_news_ratio               â”‚
â”‚     - news_embedding_768d (BERT)                             â”‚
â”‚                                                               â”‚
â”‚  â‘¢ Time-Series Featuresï¼ˆæ™‚ç³»åˆ—ç‰¹å¾´é‡ï¼‰                        â”‚
â”‚     - score_trend_30d, news_frequency_30d                    â”‚
â”‚                                                               â”‚
â”‚  â‘£ Quality Featuresï¼ˆå“è³ªç‰¹å¾´é‡ï¼‰                              â”‚
â”‚     - data_completeness, data_freshness                      â”‚
â”‚     - source_reliability                                     â”‚
â”‚                                                               â”‚
â”‚  â‘¤ Cost Estimations                                            â”‚
â”‚     - Google Custom Search API: 1æ—¥100ä»¶ç„¡æ–™, ä»¥é™$5/1000req   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Layer 3: ML/DL Modelsï¼ˆ3ã¤ã®ãƒ¢ãƒ‡ãƒ«ï¼‰                  â”‚
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ â‘  LightGBMï¼ˆãƒ¡ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ï¼‰                            â”‚   â”‚
â”‚  â”‚    - è¡¨å½¢å¼ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å°å…¥ç¢ºåº¦ã‚’äºˆæ¸¬                    â”‚   â”‚
â”‚  â”‚    - é‡ã¿: 50%                                        â”‚   â”‚
â”‚  â”‚    - ç‰¹å¾´é‡é‡è¦åº¦ã§èª¬æ˜å¯èƒ½æ€§ã‚’æ‹…ä¿                    â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ â‘¡ LSTMï¼ˆPyTorchï¼‰ã€Phase 5ã€‘                          â”‚   â”‚
â”‚  â”‚    - æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰å°†æ¥ã®ã‚¹ã‚³ã‚¢æ¨ç§»ã‚’äºˆæ¸¬            â”‚   â”‚
â”‚  â”‚    - é‡ã¿: 20%                                        â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚ â‘¢ BERTï¼ˆãƒ†ã‚­ã‚¹ãƒˆåˆ†æï¼‰ã€Phase 5ã€‘                     â”‚   â”‚
â”‚  â”‚    - ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ»äºˆç®—æ›¸ã®æ„å‘³çš„åˆ†æ                      â”‚   â”‚
â”‚  â”‚    - é‡ã¿: 30%                                        â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Layer 4: Ensemble & Explainability                   â”‚
â”‚                                                               â”‚
â”‚  - åŠ é‡å¹³å‡ã§ã‚¹ã‚³ã‚¢ã‚’çµ±åˆ                                      â”‚
â”‚  - ä¿¡é ¼åº¦è¨ˆç®—ï¼ˆãƒ¢ãƒ‡ãƒ«é–“ã®ä¸€è‡´åº¦ï¼‰                              â”‚
â”‚  - SHAP values ã§èª¬æ˜ç”Ÿæˆ                                     â”‚
â”‚  - é–¾å€¤è¶…ãˆã§Zoom Team Chaté€šçŸ¥                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Layer 5: Score Outputï¼ˆ0-100ç‚¹ï¼‰                     â”‚
â”‚                                                               â”‚
â”‚  {                                                            â”‚
â”‚    "score_total": 85.3,                                       â”‚
â”‚    "score_tabular": 82.0,                                     â”‚
â”‚    "score_timeseries": 88.0,                                  â”‚
â”‚    "score_text": 87.5,                                        â”‚
â”‚    "confidence": 0.92,                                        â”‚
â”‚    "feature_importance": {                                    â”‚
â”‚      "dx_budget_trend": 0.25,                                 â”‚
â”‚      "sentiment_score_recent": 0.18,                          â”‚
â”‚      ...                                                      â”‚
â”‚    },                                                         â”‚
â”‚    "explanation": "DXäºˆç®—ã®å¢—åŠ ãƒˆãƒ¬ãƒ³ãƒ‰ã¨ãƒã‚¸ãƒ†ã‚£ãƒ–ãªãƒ‹ãƒ¥ãƒ¼ã‚¹..." â”‚
â”‚  }                                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š Feature Engineeringï¼ˆç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ï¼‰

### 1. æ§‹é€ åŒ–ç‰¹å¾´é‡ï¼ˆTabular Featuresï¼‰

#### A. è‡ªæ²»ä½“åŸºæœ¬æƒ…å ±ï¼ˆ`municipalities`ãƒ†ãƒ¼ãƒ–ãƒ«ï¼‰

```python
# äººå£ãƒ»è¦æ¨¡ç³»ï¼ˆæ­£è¦åŒ–å¿…è¦ï¼‰
population_log = log(population)  # äººå£ã®å¯¾æ•°å¤‰æ›ï¼ˆæ­£è¦åˆ†å¸ƒã«è¿‘ã¥ã‘ã‚‹ï¼‰
population_density = population / area_km2  # äººå£å¯†åº¦
households_per_capita = households / population  # ä¸–å¸¯æ¯”ç‡

# ã‚¨ãƒªã‚¢ç‰¹æ€§ï¼ˆOne-Hot Encodingï¼‰
region_encoded = [åŒ—æµ·é“=1/0, æ±åŒ—=1/0, ..., ä¹å·=1/0]  # åœ°æ–¹ãƒ€ãƒŸãƒ¼å¤‰æ•°
prefecture_encoded = 47éƒ½é“åºœçœŒã®ã‚«ãƒ†ã‚´ãƒªå¤‰æ•°ï¼ˆTarget Encodingæ¨å¥¨ï¼‰
```

**æ ¹æ‹ **:
- å¤§è¦æ¨¡è‡ªæ²»ä½“ï¼ˆæ±äº¬23åŒºã€æ”¿ä»¤æŒ‡å®šéƒ½å¸‚ï¼‰ã¯å°å…¥ç¢ºåº¦ãŒé«˜ã„å‚¾å‘
- åœ°æ–¹ã«ã‚ˆã£ã¦ãƒ‡ã‚¸ã‚¿ãƒ«åŒ–ã®é€²æ—ã«å·®ãŒã‚ã‚‹

#### B. äºˆç®—ãƒ‡ãƒ¼ã‚¿ï¼ˆ`budgets`ãƒ†ãƒ¼ãƒ–ãƒ«ï¼‰

```python
# ç›´è¿‘3å¹´é–“ã®äºˆç®—æ¨ç§»
dx_budget_trend = (budget_2024 - budget_2022) / budget_2022  # DXäºˆç®—ã®å¢—æ¸›ç‡
total_dx_budget_3y = sum([budget_2022, budget_2023, budget_2024])  # ç´¯è¨ˆäºˆç®—
budget_allocation_ratio = dx_budget / total_budget  # DXäºˆç®—æ¯”ç‡
has_supplementary_budget = 1 if supplementary_budget > 0 else 0  # è£œæ­£äºˆç®—ãƒ•ãƒ©ã‚°

# ã‚«ãƒ†ã‚´ãƒªåˆ¥äºˆç®—
budget_by_category = {
    'åƒãæ–¹æ”¹é©': amount,
    'çª“å£DX': amount,
    'BCP': amount,
    'é˜²ç½': amount,
    'åºå†…ICT': amount,
    'é éš”æˆæ¥­': amount
}

# é®®åº¦ã‚¹ã‚³ã‚¢
budget_recency_score = {
    1ãƒ¶æœˆä»¥å†…: 1.0,
    1å¹´ä»¥å†…: 0.5,
    ãã‚Œä»¥ä¸Š: 0.2
}
```

**æ ¹æ‹ **:
- äºˆç®—å¢—é¡ãƒˆãƒ¬ãƒ³ãƒ‰ã¯å°å…¥æ„æ¬²ã®å¼·ã„ã‚·ã‚°ãƒŠãƒ«
- è£œæ­£äºˆç®—ã¯ç·Šæ€¥æ€§ãƒ»å„ªå…ˆåº¦ãŒé«˜ã„

#### C. å…¥æœ­ãƒ‡ãƒ¼ã‚¿ï¼ˆ`tenders`ãƒ†ãƒ¼ãƒ–ãƒ«ï¼‰

```python
# å…¥æœ­å®Ÿç¸¾
tender_count_1y = count(tenders WHERE date >= now() - 365 days)
tender_amount_total = sum(amount_yen)
days_since_last_tender = (today - max(tender_date))
has_zoom_related_tender = 1 if 'Zoom' in tender_title else 0
webconf_tender_count = count(tenders WHERE category='Webä¼šè­°')
```

**æ ¹æ‹ **:
- å…¥æœ­æ´»å‹•ã®å¤šã• = ç©æ¥µçš„ãªDXæ¨é€²
- Zoomé–¢é€£ã®å…¥æœ­å±¥æ­´ã¯å¼·ã„å°å…¥ã‚·ã‚°ãƒŠãƒ«

#### D. æ™‚ç³»åˆ—ç‰¹å¾´é‡ï¼ˆ`scores`ãƒ†ãƒ¼ãƒ–ãƒ« + TimescaleDBï¼‰

```python
# ã‚¹ã‚³ã‚¢æ¨ç§»
score_trend_30d = linear_regression_slope(scores[-30days])  # 30æ—¥é–“ã®ãƒˆãƒ¬ãƒ³ãƒ‰
score_volatility = std_dev(scores[-30days])  # ã‚¹ã‚³ã‚¢ã®ä¸å®‰å®šæ€§
score_momentum = score_today - score_7days_ago  # å‹¢ã„

# æ´»å‹•é »åº¦
news_frequency_30d = count(news_statements WHERE published_at >= now() - 30 days)
budget_update_frequency = count(budget_updates[-1year])
```

**æ ¹æ‹ **:
- ã‚¹ã‚³ã‚¢ãŒä¸Šæ˜‡ãƒˆãƒ¬ãƒ³ãƒ‰ã®è‡ªæ²»ä½“ã¯æ¤œè¨ãƒ•ã‚§ãƒ¼ã‚ºã«å…¥ã£ã¦ã„ã‚‹å¯èƒ½æ€§
- ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨€åŠé »åº¦ã®é«˜ã•ã¯é–¢å¿ƒã®é«˜ã•ã‚’ç¤ºã™

### 2. ãƒ†ã‚­ã‚¹ãƒˆç‰¹å¾´é‡ï¼ˆText Featuresï¼‰

#### A. ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ»ç™ºè¨€ï¼ˆ`news_statements`ãƒ†ãƒ¼ãƒ–ãƒ«ï¼‰

```python
# æ—¢å­˜LLMåˆ†æçµæœã‚’æ´»ç”¨
sentiment_score_avg = mean(sentiment_score WHERE published_at >= now() - 90 days)
sentiment_score_recent = mean(sentiment_score WHERE published_at >= now() - 30 days)
positive_news_ratio = count(sentiment > 0) / count(news)

# ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰å‡ºç¾é »åº¦ï¼ˆTF-IDFï¼‰
dx_keyword_score = tfidf_weighted_score([
    "DX", "ãƒ‡ã‚¸ã‚¿ãƒ«åŒ–", "Webä¼šè­°", "ã‚ªãƒ³ãƒ©ã‚¤ãƒ³", "ãƒ†ãƒ¬ãƒ¯ãƒ¼ã‚¯",
    "Zoom", "é éš”", "ãƒªãƒ¢ãƒ¼ãƒˆ", "ãƒ“ãƒ‡ã‚ªä¼šè­°"
])

# BERT embeddingï¼ˆPhase 5ã§å®Ÿè£…ï¼‰
news_embedding_768d = BERT.encode(concatenate(news_titles))
```

**æ ¹æ‹ **:
- ã‚»ãƒ³ãƒãƒ¡ãƒ³ãƒˆã‚¹ã‚³ã‚¢ãŒãƒã‚¸ãƒ†ã‚£ãƒ– = å°å…¥ã«å‰å‘ã
- ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®å¤šæ§˜æ€§ã¨å‡ºç¾é »åº¦ã¯é–¢å¿ƒã®æ·±ã•ã‚’ç¤ºã™

#### B. äºˆç®—æ›¸ãƒ»å…¥æœ­æ›¸é¡ã®ãƒ†ã‚­ã‚¹ãƒˆï¼ˆ`extracted_text`ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ï¼‰

```python
# LLMã«ã‚ˆã‚‹æ„å›³åˆ†é¡ï¼ˆPhase 2ã§å®Ÿè£…ï¼‰
budget_intent = LLM.classify(budget_text) â†’ {æ¤œè¨ä¸­, äºˆç®—è¨ˆä¸Š, å…¥æœ­å…¬å‘Š, å°å…¥æ¸ˆã¿}
```

### 3. ãƒ‡ãƒ¼ã‚¿å“è³ªç‰¹å¾´é‡ï¼ˆQuality Featuresï¼‰

```python
# "Garbage in, Garbage out"å¯¾ç­–
data_completeness = 1 - (null_count / total_fields)  # æ¬ æå€¤ã®å‰²åˆ
data_freshness = {
    Sãƒ©ãƒ³ã‚¯ï¼ˆ1ãƒ¶æœˆä»¥å†…ï¼‰: 1.0,
    Aãƒ©ãƒ³ã‚¯ï¼ˆ1å¹´ä»¥å†…ï¼‰: 0.7,
    ãã‚Œä»¥ä¸‹: 0.3
}
source_reliability = {
    è‡ªæ²»ä½“å…¬å¼ã‚µã‚¤ãƒˆ: 1.0,
    J-LIS: 0.9,
    ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚µã‚¤ãƒˆ: 0.7
}
```

**æ ¹æ‹ **:
- ãƒ‡ãƒ¼ã‚¿ãŒå¤ã„ãƒ»æ¬ æãŒå¤šã„å ´åˆã¯äºˆæ¸¬ç²¾åº¦ãŒä¸‹ãŒã‚‹
- å“è³ªã‚¹ã‚³ã‚¢ã‚’ç‰¹å¾´é‡ã¨ã—ã¦å«ã‚ã‚‹ã“ã¨ã§ã€ãƒ¢ãƒ‡ãƒ«ãŒä¸ç¢ºå®Ÿæ€§ã‚’è€ƒæ…®ã§ãã‚‹

---

## ğŸ¤– Machine Learning Modelsï¼ˆæ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ï¼‰

### Model 1: LightGBMï¼ˆãƒ¡ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ã€Phase 1-4ã§å®Ÿè£…ï¼‰

#### é¸å®šç†ç”±
1. **è¡¨å½¢å¼ãƒ‡ãƒ¼ã‚¿ã«æœ€é©**: è‡ªæ²»ä½“ã®æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ï¼ˆäººå£ã€äºˆç®—ã€å…¥æœ­ï¼‰ã«å¼·ã„
2. **æ¬ æå€¤ã«å¼·ã„**: ãƒ‡ãƒ¼ã‚¿ä¸å®Œå…¨ã§ã‚‚å­¦ç¿’å¯èƒ½
3. **é«˜é€Ÿæ¨è«–**: Lenovo Tinyã®CPUã§ååˆ†é«˜é€Ÿï¼ˆ1,741è‡ªæ²»ä½“ < 1ç§’ï¼‰
4. **èª¬æ˜å¯èƒ½æ€§**: ç‰¹å¾´é‡é‡è¦åº¦ãŒæ˜ç¢º â†’ å–¶æ¥­éƒ¨é–€ã¸ã®èª¬æ˜ãŒå®¹æ˜“
5. **å®Ÿç¸¾**: Kaggleã‚³ãƒ³ãƒšã§å¤šæ•°ã®å„ªå‹å®Ÿç¸¾

#### å®Ÿè£…è©³ç´°

```python
# backend/ml/models/score_predictor.py
import lightgbm as lgb
from sklearn.model_selection import train_test_split, cross_val_score

class MunicipalityScorePredictor:
    def __init__(self):
        self.model = lgb.LGBMRegressor(
            objective='regression',  # å›å¸°å•é¡Œï¼ˆ0-100ç‚¹ï¼‰
            n_estimators=500,        # æ±ºå®šæœ¨ã®æ•°
            learning_rate=0.05,      # å­¦ç¿’ç‡
            max_depth=7,             # æœ¨ã®æ·±ã•ï¼ˆéå­¦ç¿’é˜²æ­¢ï¼‰
            num_leaves=31,           # ãƒªãƒ¼ãƒ•æ•°
            min_child_samples=20,    # æœ€å°ã‚µãƒ³ãƒ—ãƒ«æ•°
            subsample=0.8,           # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ¯”ç‡
            colsample_bytree=0.8,    # ç‰¹å¾´é‡ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
            reg_alpha=0.1,           # L1æ­£å‰‡åŒ–
            reg_lambda=0.1,          # L2æ­£å‰‡åŒ–
            random_state=42
        )

    def train(self, X_train, y_train, X_val=None, y_val=None):
        """æ•™å¸«ãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’"""
        # LightGBM 4.xå¯¾å¿œ: callbacksã‚’ä½¿ç”¨
        callbacks = [
            lgb.early_stopping(stopping_rounds=50, verbose=100),
            lgb.log_evaluation(100)
        ]
        
        self.model.fit(
            X_train, y_train,
            eval_set=[(X_val, y_val)] if X_val is not None else None,
            callbacks=callbacks
        )
        logger.info(f"Training completed. Feature importance saved.")

    def predict(self, X) -> np.ndarray:
        """ã‚¹ã‚³ã‚¢äºˆæ¸¬ï¼ˆ0-100ï¼‰"""
        raw_predictions = self.model.predict(X)
        return np.clip(raw_predictions, 0, 100)  # 0-100ã«åˆ¶é™

    def get_feature_importance(self) -> Dict[str, float]:
        """ç‰¹å¾´é‡é‡è¦åº¦ã®å–å¾—"""
        importances = dict(zip(
            self.feature_names,
            self.model.feature_importances_
        ))
        # é™é †ã‚½ãƒ¼ãƒˆ
        return dict(sorted(importances.items(), key=lambda x: x[1], reverse=True))
```

#### å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ä½œæˆæˆ¦ç•¥

**Phase 1: åˆæœŸæ•™å¸«ãƒ‡ãƒ¼ã‚¿ï¼ˆCold Startï¼‰**

```python
# backend/ml/training/labeling.py
def create_initial_training_data():
    """åˆæœŸæ•™å¸«ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆ"""

    # 1. å®Ÿç¸¾ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰é€†ç®—
    labels = []

    # Zoomå°å…¥æ¸ˆã¿è‡ªæ²»ä½“ï¼ˆéå»ã®å–¶æ¥­è¨˜éŒ²ã‹ã‚‰ï¼‰
    adopted_municipalities = get_adopted_municipalities()
    for muni_id in adopted_municipalities:
        labels.append({'municipality_id': muni_id, 'score': random.uniform(90, 100)})

    # å•ã„åˆã‚ã›å±¥æ­´ã‚ã‚Š
    inquiry_municipalities = get_inquiry_municipalities()
    for muni_id in inquiry_municipalities:
        labels.append({'municipality_id': muni_id, 'score': random.uniform(70, 85)})

    # 2. ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ãƒ©ãƒ™ãƒªãƒ³ã‚°ï¼ˆé›¢æ•£çš„ãªãƒ©ãƒ³ã‚¯ä»˜ã‘ï¼‰
    all_municipalities = get_all_municipalities()
    for muni in all_municipalities:
        if has_budget_approval(muni.id):
            # Sãƒ©ãƒ³ã‚¯: äºˆç®—ãƒ»å…¥æœ­ã‚ã‚Š
            labels.append({'municipality_id': muni.id, 'score_class': 'S', 'score_value': 95.0})
        elif has_dx_news(muni.id, keywords=['æ¤œè¨', 'ãƒ‘ã‚¤ãƒ­ãƒƒãƒˆ']):
            # Aãƒ©ãƒ³ã‚¯: å…·ä½“çš„æ¤œè¨
            labels.append({'municipality_id': muni.id, 'score_class': 'A', 'score_value': 75.0})
        elif has_dx_news(muni.id, keywords=['DX', 'ãƒ‡ã‚¸ã‚¿ãƒ«']):
            # Bãƒ©ãƒ³ã‚¯: ä¸€èˆ¬çš„é–¢å¿ƒ
            labels.append({'municipality_id': muni.id, 'score_class': 'B', 'score_value': 50.0})
        else:
            # Cãƒ©ãƒ³ã‚¯: ä½é–¢å¿ƒ
            labels.append({'municipality_id': muni.id, 'score_class': 'C', 'score_value': 20.0})

    return labels
```

**Phase 2: Human-in-the-loopï¼ˆç¶™ç¶šå­¦ç¿’ï¼‰**

```python
# backend/ml/feedback/feedback_loop.py
class FeedbackLoop:
    def record_ae_feedback(
        self,
        municipality_id: int,
        predicted_score: float,
        actual_outcome: str,  # 'won', 'lost', 'in_progress'
        ae_rating: int,       # 1-5æ®µéšè©•ä¾¡ï¼ˆæ–°ãŸã«è¿½åŠ ï¼‰
        ae_comment: str
    ):
        """å–¶æ¥­æ‹…å½“è€…ï¼ˆAEï¼‰ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’è¨˜éŒ²"""
        
        # AEè©•ä¾¡ã‚’ã‚¹ã‚³ã‚¢ã«å¤‰æ› (1=20, 2=40, 3=60, 4=80, 5=100)
        actual_score = ae_rating * 20.0

        # DBã«ä¿å­˜ï¼ˆæ–°ã—ã„æ•™å¸«ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦ï¼‰
        self.db.insert('training_data', {
            'municipality_id': municipality_id,
            'score_label': actual_score,
            'predicted_score': predicted_score,
            'label_source': 'ae_feedback_v2',  # ã‚½ãƒ¼ã‚¹åŒºåˆ¥
            'ae_comment': ae_comment,
            'created_at': datetime.now()
        })
        
        # ... (é€šçŸ¥ãƒ­ã‚¸ãƒƒã‚¯ã¯ç¶™ç¶š)
```

### Model 2: LSTMï¼ˆPhase 5ã§å®Ÿè£…ã€ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰

#### ç”¨é€”
ã‚¹ã‚³ã‚¢ã®**å°†æ¥äºˆæ¸¬**ï¼ˆ7æ—¥å¾Œã€30æ—¥å¾Œã®è¦‹è¾¼ã¿ï¼‰

```python
# backend/ml/models/timeseries_predictor.py
import torch
import torch.nn as nn

class ScoreLSTM(nn.Module):
    def __init__(self, input_size=10, hidden_size=64, num_layers=2):
        super().__init__()
        self.lstm = nn.LSTM(
            input_size, hidden_size, num_layers,
            batch_first=True, dropout=0.2
        )
        self.fc = nn.Linear(hidden_size, 1)

    def forward(self, x):
        # x: (batch, seq_len, features) - ä¾‹: (batch, 30æ—¥åˆ†, 10ç‰¹å¾´é‡)
        lstm_out, _ = self.lstm(x)
        last_output = lstm_out[:, -1, :]  # æœ€å¾Œã®ã‚¿ã‚¤ãƒ ã‚¹ãƒ†ãƒƒãƒ—
        score = self.fc(last_output)
        return torch.sigmoid(score) * 100  # 0-100ã«ã‚¹ã‚±ãƒ¼ãƒ«
```

### Model 3: BERTï¼ˆPhase 5ã§å®Ÿè£…ã€ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰

#### ç”¨é€”
ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ»äºˆç®—æ›¸ã®**æ„å‘³çš„åˆ†æ**

```python
# backend/ml/models/text_analyzer.py
from transformers import AutoTokenizer, AutoModel

class BERTTextAnalyzer:
    def __init__(self, model_name="cl-tohoku/bert-base-japanese-v3"):
        """æ—¥æœ¬èªBERTãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–"""
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModel.from_pretrained(model_name)
        self.model.eval()  # æ¨è«–ãƒ¢ãƒ¼ãƒ‰

    def encode_text(self, text: str) -> np.ndarray:
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’768æ¬¡å…ƒãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ›"""
        inputs = self.tokenizer(
            text,
            return_tensors="pt",
            max_length=512,
            truncation=True,
            padding=True
        )

        with torch.no_grad():
            outputs = self.model(**inputs)
            # [CLS]ãƒˆãƒ¼ã‚¯ãƒ³ã®åŸ‹ã‚è¾¼ã¿ã‚’ä½¿ç”¨
            embedding = outputs.last_hidden_state[:, 0, :].numpy()

        return embedding.flatten()  # (768,)

    def semantic_similarity(self, text1: str, text2: str) -> float:
        """2ã¤ã®ãƒ†ã‚­ã‚¹ãƒˆã®æ„å‘³çš„é¡ä¼¼åº¦ï¼ˆã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ï¼‰"""
        emb1 = self.encode_text(text1)
        emb2 = self.encode_text(text2)
        return np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))
```

---

## ğŸ”— Ensemble Strategyï¼ˆã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«æˆ¦ç•¥ï¼‰

### ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«è¨ˆç®—

```python
# backend/ml/ensemble/score_calculator.py
class EnsembleScoreCalculator:
    def __init__(self):
        self.lgb_model = MunicipalityScorePredictor()
        self.lstm_model = ScoreLSTM()  # Phase 5
        self.bert_analyzer = BERTTextAnalyzer()  # Phase 5

        # é‡ã¿ä»˜ã‘ï¼ˆåˆæœŸå€¤ï¼‰
        # å°†æ¥çš„ã«ã¯ Bayesian Optimization ç­‰ã§æœ€é©åŒ–
        self.weights = {
            'tabular': 0.5,      # LightGBM
            'timeseries': 0.2,   # LSTM
            'text': 0.3          # BERT
        }

    def calculate_final_score(
        self,
        tabular_features: np.ndarray,
        timeseries_features: np.ndarray = None,
        text_features: List[str] = None
    ) -> Dict[str, Any]:
        """æœ€çµ‚ã‚¹ã‚³ã‚¢è¨ˆç®—"""

        # å„ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬
        score_tabular = self.lgb_model.predict(tabular_features)[0]

        # Phase 1-4ã§ã¯LightGBMã®ã¿
        if timeseries_features is None or text_features is None:
            return {
                'score_total': float(score_tabular),
                'score_tabular': float(score_tabular),
                'confidence': 1.0,
                'model_weights': {'tabular': 1.0},
                'feature_importance': self.lgb_model.get_feature_importance()
            }

        # Phase 5: å…¨ãƒ¢ãƒ‡ãƒ«çµ±åˆ
        score_timeseries = self.lstm_model(timeseries_features).item()
        score_text = self._calculate_text_score(text_features)

        # åŠ é‡å¹³å‡
        final_score = (
            self.weights['tabular'] * score_tabular +
            self.weights['timeseries'] * score_timeseries +
            self.weights['text'] * score_text
        )

        # ä¿¡é ¼åº¦è¨ˆç®—ï¼ˆãƒ¢ãƒ‡ãƒ«é–“ã®ä¸€è‡´åº¦ï¼‰
        confidence = self._calculate_confidence(
            score_tabular, score_timeseries, score_text
        )

        return {
            'score_total': float(final_score),
            'score_tabular': float(score_tabular),
            'score_timeseries': float(score_timeseries),
            'score_text': float(score_text),
            'confidence': float(confidence),
            'model_weights': self.weights,
            'feature_importance': self.lgb_model.get_feature_importance()
        }

    def _calculate_confidence(self, *scores) -> float:
        """ãƒ¢ãƒ‡ãƒ«é–“ã®ä¸€è‡´åº¦ã¨å„ãƒ¢ãƒ‡ãƒ«ã®ä¸ç¢ºå®Ÿæ€§ã‹ã‚‰ä¿¡é ¼åº¦ã‚’è¨ˆç®—"""
        std_dev = np.std(scores)
        
        # ãƒ¢ãƒ‡ãƒ«ã”ã¨ã®åˆ†æ•£ã‚‚è€ƒæ…®ã™ã¹ãã ãŒã€ã¾ãšã¯ç°¡æ˜“å®Ÿè£…
        # åˆ†æ•£ãŒå¤§ãã„ = ãƒ¢ãƒ‡ãƒ«é–“ã§æ„è¦‹ãŒå‰²ã‚Œã¦ã„ã‚‹ = ä¿¡é ¼åº¦ä½
        base_confidence = max(0.0, 1.0 - (std_dev / 25.0)) # å³ã—ã‚ã«è¨­å®š
        
        return base_confidence
```

---

## ğŸ§ª Data Quality Checkï¼ˆ"Garbage in, Garbage out"å¯¾ç­–ï¼‰

### ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯æ©Ÿæ§‹

```python
# backend/ml/data/data_quality.py
from sklearn.ensemble import IsolationForest

class DataQualityChecker:
    async def check_data_quality(self, df: pd.DataFrame) -> Dict[str, Any]:
        """ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯ (Async)"""

        quality_report = {
            'completeness': self._check_completeness(df),
            'consistency': self._check_consistency(df),
            'freshness': self._check_freshness(df),
            'outliers': self._detect_outliers(df),
            'duplicates': self._check_duplicates(df)
        }

        # å“è³ªã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆ0.0-1.0ï¼‰
        quality_score = (
            quality_report['completeness'] * 0.3 +
            quality_report['consistency'] * 0.3 +
            quality_report['freshness'] * 0.4
        )

        quality_report['overall_score'] = quality_score

        # é–¾å€¤ã‚’ä¸‹å›ã‚‹å ´åˆã¯Zoom Team Chatã«ã‚¢ãƒ©ãƒ¼ãƒˆ
        if quality_score < 0.7:
            await self._send_alert_to_zoom_chat(quality_report)

        return quality_report

    def _check_completeness(self, df: pd.DataFrame) -> float:
        """æ¬ æå€¤ãƒã‚§ãƒƒã‚¯"""
        total_values = df.shape[0] * df.shape[1]
        missing_values = df.isnull().sum().sum()
        return 1 - (missing_values / total_values)

    def _check_freshness(self, df: pd.DataFrame) -> float:
        """ãƒ‡ãƒ¼ã‚¿é®®åº¦ãƒã‚§ãƒƒã‚¯ï¼ˆSãƒ©ãƒ³ã‚¯/Aãƒ©ãƒ³ã‚¯ï¼‰"""
        if 'updated_at' not in df.columns:
            return 0.5

        now = datetime.now()
        df['days_old'] = (now - pd.to_datetime(df['updated_at'])).dt.days

        # é®®åº¦ã‚¹ã‚³ã‚¢è¨ˆç®—
        freshness_scores = df['days_old'].apply(lambda d:
            1.0 if d <= 30 else  # Sãƒ©ãƒ³ã‚¯ï¼ˆ1ãƒ¶æœˆä»¥å†…ï¼‰
            0.7 if d <= 365 else  # Aãƒ©ãƒ³ã‚¯ï¼ˆ1å¹´ä»¥å†…ï¼‰
            0.3  # ãã‚Œä»¥ä¸‹
        )

        return freshness_scores.mean()

    def _detect_outliers(self, df: pd.DataFrame) -> List[Dict]:
        """ç•°å¸¸å€¤æ¤œå‡ºï¼ˆIsolation Forestï¼‰"""
        numeric_cols = df.select_dtypes(include=[np.number]).columns

        if len(numeric_cols) == 0:
            return []

        clf = IsolationForest(contamination=0.1, random_state=42)
        outliers = clf.fit_predict(df[numeric_cols].fillna(0))

        outlier_indices = df[outliers == -1].index.tolist()

        return [{
            'index': idx,
            'municipality_id': df.loc[idx, 'id'],
            'reason': 'çµ±è¨ˆçš„ç•°å¸¸å€¤'
        } for idx in outlier_indices]

    async def _send_alert_to_zoom_chat(self, quality_report: Dict):
        """Zoom Team Chatã«ã‚¢ãƒ©ãƒ¼ãƒˆé€ä¿¡"""
        message = f"""
âš ï¸ **ãƒ‡ãƒ¼ã‚¿å“è³ªã‚¢ãƒ©ãƒ¼ãƒˆ**

ç·åˆå“è³ªã‚¹ã‚³ã‚¢: {quality_report['overall_score']:.2f}
- å®Œå…¨æ€§: {quality_report['completeness']:.2f}
- ä¸€è²«æ€§: {quality_report['consistency']:.2f}
- é®®åº¦: {quality_report['freshness']:.2f}

ç•°å¸¸å€¤æ¤œå‡º: {len(quality_report['outliers'])}ä»¶

å¯¾å¿œãŒå¿…è¦ã§ã™ã€‚
        """
        await send_to_zoom_team_chat(message)
```

---

## ğŸš€ API Implementationï¼ˆAPIå®Ÿè£…ï¼‰

### ãƒãƒƒãƒæ¨è«–ã‚·ã‚¹ãƒ†ãƒ 

```python
# backend/ml/inference/batch_predictor.py
class BatchScorePredictor:
    """å…¨1,741è‡ªæ²»ä½“ã®ã‚¹ã‚³ã‚¢ã‚’å¤œé–“ãƒãƒƒãƒã§è¨ˆç®—"""

    async def run_nightly_scoring(self):
        """æ¯æ™©3æ™‚ã«å®Ÿè¡Œï¼ˆCronè¨­å®šï¼‰"""
        start_time = datetime.now()
        logger.info("ğŸŒ™ Nightly ML scoring started")

        # 1. ãƒ‡ãƒ¼ã‚¿åé›†
        municipalities = await self.fetch_all_municipalities()
        logger.info(f"Fetched {len(municipalities)} municipalities")

        # 2. ç‰¹å¾´é‡ç”Ÿæˆï¼ˆä¸¦åˆ—å‡¦ç†ï¼‰
        features_df = await self.generate_features_parallel(municipalities)
        logger.info(f"Generated features: {features_df.shape}")

        # 3. ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯
        quality_report = await self.quality_checker.check_data_quality(features_df)
        logger.info(f"Data quality score: {quality_report['overall_score']:.2f}")

        # å“è³ªãŒä½ã„å ´åˆã¯è­¦å‘Š
        if quality_report['overall_score'] < 0.7:
            logger.warning("âš ï¸ Data quality below threshold!")

        # 4. MLã‚¹ã‚³ã‚¢è¨ˆç®— (ãƒãƒƒãƒæ¨è«–)
        # DataFrameã”ã¨æ¸¡ã—ã¦é«˜é€ŸåŒ–
        tabular_matrix = features_df[self.feature_cols].values
        raw_scores = self.ensemble_calculator.lgb_model.predict(tabular_matrix)
        
        scores = []
        for idx, (muni_id, score_val) in enumerate(zip(features_df['id'], raw_scores)):
             scores.append({
                'municipality_id': muni_id,
                'score_total': float(score_val),
                'confidence': 1.0, # Phase 1-4
                'feature_importance': {} # å€‹åˆ¥é‡è¦åº¦ã¯é‡ã„ã®ã§çœç•¥ã¾ãŸã¯ä»£è¡¨å€¤ã®ã¿
            })

        # 5. DBä¿å­˜
        await self.save_scores_to_db(scores)
        logger.info(f"Saved {len(scores)} scores to database")

        # 6. Zoom Team Chaté€šçŸ¥
        duration = (datetime.now() - start_time).total_seconds()
        await self.notify_completion(duration, quality_report, len(scores))

        logger.info(f"âœ… Nightly scoring completed in {duration:.2f}s")
```

### MLã‚¹ã‚³ã‚¢API

```python
# backend/routers/ml_scores.py
from fastapi import APIRouter, Depends, HTTPException
from sqlalchemy.orm import Session

router = APIRouter(prefix='/api/ml-scores', tags=['ML Scoring'])

@router.get("/{municipality_id}")
async def get_ml_score(municipality_id: int, db: Session = Depends(get_db)):
    """MLãƒ¢ãƒ‡ãƒ«ã§ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ï¼ˆãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ï¼‰"""

    # Redisã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
    cached = await redis.get(f"ml_score:{municipality_id}")
    if cached:
        logger.info(f"Cache hit for municipality {municipality_id}")
        return json.loads(cached)

    # å†è¨ˆç®—
    service = MLScoreService()
    score = await service.calculate_score_realtime(municipality_id)

    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ï¼ˆ1æ™‚é–“ï¼‰
    await redis.setex(
        f"ml_score:{municipality_id}",
        3600,
        json.dumps(score)
    )

    return score

@router.post("/calculate/batch")
async def batch_calculate(
    region: Optional[str] = None,
    db: Session = Depends(get_db)
):
    """å…¨1,741è‡ªæ²»ä½“ã®ã‚¹ã‚³ã‚¢ã‚’ä¸€æ‹¬è¨ˆç®—ï¼ˆå¤œé–“ãƒãƒƒãƒç”¨ï¼‰"""

    service = MLScoreService()
    results = await service.batch_calculate_all_municipalities(region=region)

    # Zoom Team Chatã¸é€šçŸ¥
    await notify_zoom_team_chat({
        'total_processed': len(results),
        'average_score': np.mean([r['score_total'] for r in results]),
        'high_score_count': len([r for r in results if r['score_total'] > 70])
    })

    return {"status": "success", "processed": len(results)}

@router.get("/data-quality")
async def get_data_quality_report(db: Session = Depends(get_db)):
    """ãƒ‡ãƒ¼ã‚¿å“è³ªãƒ¬ãƒãƒ¼ãƒˆã‚’å–å¾—"""
    checker = DataQualityChecker()
    municipalities_df = await load_municipalities_with_features(db)
    report = checker.check_data_quality(municipalities_df)
    return report

@router.get("/feature-importance")
async def get_feature_importance():
    """ç‰¹å¾´é‡é‡è¦åº¦ã‚’å–å¾—ï¼ˆå–¶æ¥­éƒ¨é–€ã¸ã®èª¬æ˜ç”¨ï¼‰"""
    predictor = MunicipalityScorePredictor()
    predictor.load_model()  # æœ€æ–°ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿
    importance = predictor.get_feature_importance()
    return importance
```

---

## ğŸ“… Implementation Scheduleï¼ˆå®Ÿè£…ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼‰

| Week | Phase | ã‚¿ã‚¹ã‚¯ | æˆæœç‰© |
|------|-------|--------|--------|
| **1-2** | **Phase 1: åŸºç›¤æ§‹ç¯‰** | - ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹æˆä½œæˆ<br>- ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼å®Ÿè£…<br>- ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ï¼ˆè¡¨å½¢å¼ã®ã¿ï¼‰<br>- ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯æ©Ÿæ§‹<br>- LightGBMãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ« | - `backend/ml/` å…¨ä½“<br>- ãƒ‡ãƒ¼ã‚¿å“è³ªãƒ¬ãƒãƒ¼ãƒˆ<br>- åˆæœŸãƒ¢ãƒ‡ãƒ«ï¼ˆaccuracyæœªæ¤œè¨¼ï¼‰ |
| **3** | **Phase 2: æ•™å¸«ãƒ‡ãƒ¼ã‚¿ä½œæˆ** | - å®Ÿç¸¾ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ©ãƒ™ãƒ«ç”Ÿæˆ<br>- ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ãƒ©ãƒ™ãƒªãƒ³ã‚°<br>- AEã«ã‚ˆã‚‹åˆæœŸãƒ©ãƒ™ãƒªãƒ³ã‚°ï¼ˆ50è‡ªæ²»ä½“ï¼‰<br>- æ•™å¸«ãƒ‡ãƒ¼ã‚¿DBãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ | - `training_data`ãƒ†ãƒ¼ãƒ–ãƒ«<br>- åˆæœŸæ•™å¸«ãƒ‡ãƒ¼ã‚¿ï¼ˆ~200ä»¶ï¼‰ |
| **4-5** | **Phase 3: MLãƒ¢ãƒ‡ãƒ«é–‹ç™º** | - LightGBMå­¦ç¿’ãƒ»è©•ä¾¡<br>- ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°<br>- äº¤å·®æ¤œè¨¼<br>- è©•ä¾¡æŒ‡æ¨™å®Ÿè£… | - å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ï¼ˆ.pklï¼‰<br>- è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆï¼ˆMAE, R2ç­‰ï¼‰ |
| **6** | **Phase 4: APIãƒ»ãƒãƒƒãƒæ¨è«–** | - MLã‚¹ã‚³ã‚¢APIã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ<br>- ãƒãƒƒãƒæ¨è«–ã‚·ã‚¹ãƒ†ãƒ <br>- Redisã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±åˆ<br>- Zoom Team Chaté€šçŸ¥ | - `/api/ml-scores` API<br>- Cronè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«<br>- å¤œé–“ãƒãƒƒãƒå‹•ä½œç¢ºèª |
| **7-8** | **Phase 5ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰** | - BERTçµ±åˆ<br>- LSTMæ™‚ç³»åˆ—äºˆæ¸¬<br>- ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«è¨ˆç®—<br>- SHAPçµ±åˆï¼ˆèª¬æ˜å¯èƒ½æ€§ï¼‰ | - 3ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«<br>- SHAPå¯è¦–åŒ– |

---

## ğŸ“¦ Dependenciesï¼ˆä¾å­˜ãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼‰

### `backend/requirements.txt`ï¼ˆè¿½åŠ ï¼‰

```txt
# === Machine Learning ===
scikit-learn>=1.3.0      # ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã€è©•ä¾¡æŒ‡æ¨™
lightgbm>=4.1.0          # ãƒ¡ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«ï¼ˆGradient Boostingï¼‰

# === Deep Learningï¼ˆPhase 5ã§å¿…è¦ï¼‰ ===
torch>=2.1.0             # LSTMå®Ÿè£…
transformers>=4.35.0     # BERTï¼ˆæ—¥æœ¬èªï¼‰
sentencepiece>=0.1.99    # BERTç”¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼

# === Data Science ===
pandas>=2.1.0            # ãƒ‡ãƒ¼ã‚¿å‡¦ç†
numpy>=1.24.0            # æ•°å€¤è¨ˆç®—

# === Model Interpretabilityï¼ˆPhase 5ã§å¿…è¦ï¼‰ ===
shap>=0.43.0             # SHAP valuesï¼ˆèª¬æ˜å¯èƒ½æ€§ï¼‰

# === Utilities ===
joblib>=1.3.0            # ãƒ¢ãƒ‡ãƒ«ã®ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚º
```

---

## âœ… Success Criteriaï¼ˆæˆåŠŸæŒ‡æ¨™ï¼‰

### KPIè¨­å®š

| æŒ‡æ¨™ | ç›®æ¨™å€¤ | æ¸¬å®šæ–¹æ³• | é‡è¦åº¦ |
|------|--------|---------|--------|
| **ãƒ¢ãƒ‡ãƒ«ç²¾åº¦ï¼ˆMAEï¼‰** | < 10ç‚¹ | ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§è©•ä¾¡ | ğŸ”´ High |
| **ä¸Šä½20ç¤¾ã®ç²¾åº¦** | > 85% | å®Ÿéš›ã®å•ã„åˆã‚ã›ç‡ã¨æ¯”è¼ƒ | ğŸ”´ High |
| **ãƒ‡ãƒ¼ã‚¿å“è³ªã‚¹ã‚³ã‚¢** | > 0.8 | "Garbage in, Garbage out"ãƒã‚§ãƒƒã‚¯ | ğŸ”´ High |
| **ãƒãƒƒãƒå‡¦ç†æ™‚é–“** | < 5åˆ† | 1,741è‡ªæ²»ä½“ã®ä¸€æ‹¬è¨ˆç®— | ğŸŸ¡ Medium |
| **ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¨è«–æ™‚é–“** | < 1ç§’ | å˜ä¸€è‡ªæ²»ä½“ã‚¹ã‚³ã‚¢è¨ˆç®— | ğŸŸ¡ Medium |
| **AEæº€è¶³åº¦** | > 4.0/5.0 | ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚¢ãƒ³ã‚±ãƒ¼ãƒˆ | ğŸŸ¢ Low |

### æ¤œè¨¼æ‰‹é †

#### 1. ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆ

```bash
# ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã®ãƒ†ã‚¹ãƒˆ
pytest backend/ml/tests/test_feature_engineer.py

# LightGBMãƒ¢ãƒ‡ãƒ«ã®ãƒ†ã‚¹ãƒˆ
pytest backend/ml/tests/test_score_predictor.py

# ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯ã®ãƒ†ã‚¹ãƒˆ
pytest backend/ml/tests/test_data_quality.py
```

#### 2. çµ±åˆãƒ†ã‚¹ãƒˆ

```bash
# å˜ä¸€è‡ªæ²»ä½“ã®ã‚¹ã‚³ã‚¢è¨ˆç®—
curl -X POST http://localhost:8000/api/ml-scores/calculate/131130

# ãƒãƒƒãƒè¨ˆç®—
curl -X POST http://localhost:8000/api/ml-scores/calculate/batch

# ãƒ‡ãƒ¼ã‚¿å“è³ªãƒ¬ãƒãƒ¼ãƒˆ
curl http://localhost:8000/api/ml-scores/data-quality
```

#### 3. æœ¬ç•ªç’°å¢ƒãƒ†ã‚¹ãƒˆï¼ˆLenovo Tinyï¼‰

```bash
# SSHæ¥ç¶š
ssh ubuntu@100.107.246.40  # TailscaleçµŒç”±

# Dockerã‚³ãƒ³ãƒ†ãƒŠç¢ºèª
docker compose ps

# ãƒ­ã‚°ç¢ºèª
docker compose logs -f api

# å¤œé–“ãƒãƒƒãƒæ‰‹å‹•å®Ÿè¡Œ
curl -X POST http://localhost:8000/api/ml-scores/calculate/batch
```

---

## âš ï¸ Risks & Mitigationï¼ˆãƒªã‚¹ã‚¯ã¨å¯¾ç­–ï¼‰

| ãƒªã‚¹ã‚¯ | å½±éŸ¿åº¦ | å¯¾ç­– |
|--------|--------|------|
| **æ•™å¸«ãƒ‡ãƒ¼ã‚¿ä¸è¶³** | ğŸ”´ High | - ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ãƒ©ãƒ™ãƒªãƒ³ã‚°ã§åˆæœŸãƒ‡ãƒ¼ã‚¿ä½œæˆ<br>- Human-in-the-loopã§ç¶™ç¶šçš„ã«å¢—ã‚„ã™<br>- é¡ä¼¼è‡ªæ²»ä½“ã‹ã‚‰ã®è»¢ç§»å­¦ç¿’ |
| **ãƒ¢ãƒ‡ãƒ«ã®éå­¦ç¿’** | ğŸŸ¡ Medium | - äº¤å·®æ¤œè¨¼ã§æ±åŒ–æ€§èƒ½ã‚’ç¢ºèª<br>- Early Stopping + L2æ­£å‰‡åŒ–<br>- ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§å®šæœŸè©•ä¾¡ |
| **Lenovo Tinyã®CPUä¸è¶³** | ğŸŸ¡ Medium | - LightGBMå„ªå…ˆï¼ˆè»½é‡ï¼‰<br>- BERTã¯å¿…è¦æ™‚ã®ã¿ä½¿ç”¨<br>- ãƒãƒƒãƒå‡¦ç†ã¯å¤œé–“å®Ÿè¡Œ |
| **ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ¨è«–ãŒé…ã„** | ğŸŸ¢ Low | - Redisã‚­ãƒ£ãƒƒã‚·ãƒ¥æ´»ç”¨<br>- å¤œé–“ãƒãƒƒãƒã§äº‹å‰è¨ˆç®— |
| **ãƒ‡ãƒ¼ã‚¿å“è³ªã®åŠ£åŒ–** | ğŸ”´ High | - å®šæœŸçš„ãªå“è³ªãƒã‚§ãƒƒã‚¯<br>- é–¾å€¤ä¸‹å›ã‚Šæ™‚ã«Zoom Team Chatã‚¢ãƒ©ãƒ¼ãƒˆ<br>- ç•°å¸¸å€¤è‡ªå‹•æ¤œå‡ºï¼ˆIsolation Forestï¼‰ |
| **ãƒ¢ãƒ‡ãƒ«ãƒ‰ãƒªãƒ•ãƒˆ** | ğŸŸ¡ Medium | - AEãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã§æ¤œçŸ¥<br>- äºˆæ¸¬ã¨å®Ÿç¸¾ã®å·®åˆ†ã‚’ç›£è¦–<br>- å®šæœŸçš„ãªå†å­¦ç¿’ |

---

## ğŸ“Œ Critical Filesï¼ˆé‡è¦ãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ï¼‰

### æ–°è¦ä½œæˆãƒ•ã‚¡ã‚¤ãƒ«

| ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ | å½¹å‰² | è¡Œæ•°ï¼ˆæ¨å®šï¼‰ |
|-------------|------|-------------|
| `backend/ml/data/data_quality.py` | ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯ | ~200è¡Œ |
| `backend/ml/features/feature_engineer.py` | ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚° | ~300è¡Œ |
| `backend/ml/models/score_predictor.py` | LightGBMãƒ¢ãƒ‡ãƒ« | ~150è¡Œ |
| `backend/ml/training/trainer.py` | ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ | ~200è¡Œ |
| `backend/ml/training/evaluator.py` | ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ | ~100è¡Œ |
| `backend/ml/training/labeling.py` | æ•™å¸«ãƒ‡ãƒ¼ã‚¿ä½œæˆ | ~200è¡Œ |
| `backend/ml/ensemble/score_calculator.py` | ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«è¨ˆç®— | ~150è¡Œ |
| `backend/ml/feedback/feedback_loop.py` | Human-in-the-loop | ~100è¡Œ |
| `backend/ml/inference/batch_predictor.py` | ãƒãƒƒãƒæ¨è«– | ~150è¡Œ |
| `backend/routers/ml_scores.py` | MLã‚¹ã‚³ã‚¢API | ~150è¡Œ |
| **åˆè¨ˆ** | | **~1,700è¡Œ** |

### ä¿®æ­£ãƒ•ã‚¡ã‚¤ãƒ«

| ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ | ä¿®æ­£å†…å®¹ | è¡Œæ•°ï¼ˆæ¨å®šï¼‰ |
|-------------|---------|-------------|
| `backend/requirements.txt` | ML/DLãƒ©ã‚¤ãƒ–ãƒ©ãƒªè¿½åŠ  | +10è¡Œ |
| `backend/main.py` | MLã‚¹ã‚³ã‚¢ãƒ«ãƒ¼ã‚¿ãƒ¼è¿½åŠ  | +5è¡Œ |
| `backend/db/init.sql` | `training_data`ãƒ†ãƒ¼ãƒ–ãƒ«è¿½åŠ  | +20è¡Œ |

### å‚ç…§ãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆä¿®æ­£ä¸è¦ï¼‰

- `backend/services/llm_analyzer.py` - æ—¢å­˜LLMåˆ†æã¨ã®çµ±åˆç‚¹
- `backend/models/municipality.py` - è‡ªæ²»ä½“ãƒ‡ãƒ¼ã‚¿ãƒ¢ãƒ‡ãƒ«
- `docs/ai_strategy.md` - AIæˆ¦ç•¥æ–‡æ›¸

---

## ğŸ“ Summaryï¼ˆã¾ã¨ã‚ï¼‰

### å®Ÿè£…ã®ç›®çš„

ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ã‚¹ãƒ»æ©Ÿæ¢°å­¦ç¿’ãƒ»ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’æ´»ç”¨ã—ã€ã€ŒGarbage in, Garbage outã€ã®åŸå‰‡ã«åŸºã¥ã„ãŸè³ªã®é«˜ã„è‡ªæ²»ä½“ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã‚’å®Ÿç¾ã—ã¾ã™ã€‚

### æŠ€è¡“çš„ç‰¹å¾´

1. **LightGBMã‚’ãƒ¡ã‚¤ãƒ³ãƒ¢ãƒ‡ãƒ«**ã¨ã—ã€è¡¨å½¢å¼ãƒ‡ãƒ¼ã‚¿ã«æœ€é©åŒ–
2. **ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°**ã§éæ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã‚’æ§‹é€ åŒ–
3. **ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯**ã§ä½å“è³ªãƒ‡ãƒ¼ã‚¿ã‚’é™¤å¤–
4. **Human-in-the-loop**ã§ç¶™ç¶šçš„ã«æ”¹å–„
5. **Zoom Team Chaté€šçŸ¥**ã§ã‚¢ãƒ©ãƒ¼ãƒˆãƒ»ãƒ¬ãƒãƒ¼ãƒˆé…ä¿¡

### å®Ÿè£…è¦æ¨¡

- **Phase 1-4ï¼ˆå¿…é ˆï¼‰**: ç´„1,700è¡Œã€æ‰€è¦æ™‚é–“ 6é€±é–“
- **Phase 5ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰**: Deep Learningçµ±åˆã€è¿½åŠ 2-3é€±é–“

### æœŸå¾…ã•ã‚Œã‚‹åŠ¹æœ

1. **å–¶æ¥­æ´»å‹•ã®åŠ¹ç‡åŒ–**: ä¸Šä½20ç¤¾ã®ç²¾åº¦ > 85%
2. **ãƒ‡ãƒ¼ã‚¿ãƒ‰ãƒªãƒ–ãƒ³ãªæ„æ€æ±ºå®š**: ç‰¹å¾´é‡é‡è¦åº¦ã§èª¬æ˜å¯èƒ½
3. **ç¶™ç¶šçš„ãªæ”¹å–„**: AEãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã§ç²¾åº¦å‘ä¸Š

---

## ğŸ” Review Checklistï¼ˆãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆï¼‰

ã“ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ãƒ¬ãƒ“ãƒ¥ãƒ¼ã™ã‚‹éš›ã¯ã€ä»¥ä¸‹ã®è¦³ç‚¹ã§è©•ä¾¡ã—ã¦ãã ã•ã„ã€‚

### Technical Feasibilityï¼ˆæŠ€è¡“çš„å®Ÿç¾å¯èƒ½æ€§ï¼‰
- [ ] LightGBMã¯æœ¬å½“ã«æœ€é©ãªãƒ¢ãƒ‡ãƒ«ã‹ï¼Ÿä»–ã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ï¼ˆXGBoostã€CatBoostï¼‰ã‚‚æ¤œè¨ã—ãŸã‹ï¼Ÿ
- [ ] Lenovo Tinyã®CPU/RAMã§1,741è‡ªæ²»ä½“ã®ãƒãƒƒãƒå‡¦ç†ã¯ç¾å®Ÿçš„ã‹ï¼Ÿ
- [ ] BERT/LSTMã®çµ±åˆã¯æœ¬å½“ã«å¿…è¦ã‹ï¼Ÿã‚³ã‚¹ãƒˆå¯¾åŠ¹æœã¯ï¼Ÿ

### Data Qualityï¼ˆãƒ‡ãƒ¼ã‚¿å“è³ªï¼‰
- [ ] "Garbage in, Garbage out"å¯¾ç­–ã¯ååˆ†ã‹ï¼Ÿ
- [ ] ç•°å¸¸å€¤æ¤œå‡ºã®æ‰‹æ³•ï¼ˆIsolation Forestï¼‰ã¯é©åˆ‡ã‹ï¼Ÿ
- [ ] é®®åº¦ã‚¹ã‚³ã‚¢ã®é–¾å€¤ï¼ˆ1ãƒ¶æœˆã€1å¹´ï¼‰ã¯å¦¥å½“ã‹ï¼Ÿ

### Labeling Strategyï¼ˆãƒ©ãƒ™ãƒªãƒ³ã‚°æˆ¦ç•¥ï¼‰
- [ ] åˆæœŸæ•™å¸«ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆæ–¹æ³•ã¯ç¾å®Ÿçš„ã‹ï¼Ÿ
- [ ] ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ãƒ©ãƒ™ãƒªãƒ³ã‚°ã®åŸºæº–ã¯é©åˆ‡ã‹ï¼Ÿ
- [ ] Human-in-the-loopã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ«ãƒ¼ãƒ—ã¯æ©Ÿèƒ½ã™ã‚‹ã‹ï¼Ÿ

### Evaluation Metricsï¼ˆè©•ä¾¡æŒ‡æ¨™ï¼‰
- [ ] MAE < 10ç‚¹ã¯é”æˆå¯èƒ½ã‹ï¼Ÿå³ã—ã™ããªã„ã‹ï¼Ÿ
- [ ] ä¸Šä½20ç¤¾ã®ç²¾åº¦ > 85%ã¯ãƒ“ã‚¸ãƒã‚¹è¦ä»¶ã«åˆã£ã¦ã„ã‚‹ã‹ï¼Ÿ
- [ ] ä»–ã«è¿½åŠ ã™ã¹ãè©•ä¾¡æŒ‡æ¨™ã¯ãªã„ã‹ï¼Ÿ

### Implementation Scheduleï¼ˆå®Ÿè£…ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼‰
- [ ] 6-8é€±é–“ã®ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã¯ç¾å®Ÿçš„ã‹ï¼Ÿ
- [ ] Phase 1-4ã§æœ¬å½“ã«å¿…è¦ååˆ†ã‹ï¼ŸPhase 5ã¯æœ¬å½“ã«ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‹ï¼Ÿ
- [ ] ãƒªã‚¹ã‚¯ãƒãƒƒãƒ•ã‚¡ã¯è€ƒæ…®ã•ã‚Œã¦ã„ã‚‹ã‹ï¼Ÿ

### Cost & Infrastructureï¼ˆã‚³ã‚¹ãƒˆãƒ»ã‚¤ãƒ³ãƒ•ãƒ©ï¼‰
- [ ] Lenovo Tinyã§ååˆ†ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãŒå‡ºã‚‹ã‹ï¼Ÿ
- [ ] Zoom Team Chaté€šçŸ¥ã®é »åº¦ã¯é©åˆ‡ã‹ï¼ˆã‚¹ãƒ‘ãƒ ã«ãªã‚‰ãªã„ã‹ï¼‰ï¼Ÿ
- [ ] ãƒ¢ãƒ‡ãƒ«ã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ç®¡ç†æˆ¦ç•¥ã¯ï¼Ÿ

---

**ãƒ¬ãƒ“ãƒ¥ãƒ¼ä¾é ¼å…ˆ**: ä»–ã®AIã€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼ã€ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚¨ãƒ³ãƒ†ã‚£ã‚¹ãƒˆ
**ãƒ¬ãƒ“ãƒ¥ãƒ¼æœŸé™**: 2026-02-15ã¾ã§
**æ¬¡å›ã‚¢ã‚¯ã‚·ãƒ§ãƒ³**: ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’åæ˜ å¾Œã€Phase 1å®Ÿè£…é–‹å§‹

**æ®‹ãƒˆãƒ¼ã‚¯ãƒ³**: 95,494 / 200,000 tokens
