# リアルタイム情報収集・高精度選別戦略

## 1. 基本方針: 「鮮度」と「網羅性」の徹底
過去の統計データ（国勢調査など）はあくまで自治体の「規模」を知るための基礎情報に過ぎません。
営業活動で真に武器になるのは、**「今、その自治体で何が起きているか」**を示すリアルタイム情報です。

### ターゲットとする情報の鮮度基準
- **Sランク（直近1ヶ月以内）**: 最新の入札公告、議会での発言録、補正予算案
- **Aランク（直近1年以内）**: 今年度の施策方針、DX推進計画書
- **論外**: 2年以上前のデータ（人口動態以外では営業に使わない）

## 2. 「集め倒す」ための収集戦略 (The Collector)
あらゆるソースから、ZoomやDXに関連するシグナルを収集します。

| 情報ソース | 収集対象 | 収集頻度 | 技術的アプローチ |
| :--- | :--- | :--- | :--- |
| **自治体Webサイト** | 新着情報、入札情報ページ | 毎日 | Python (Scrapy/Selenium) + 更新検知 |
| **調達情報ポータル** | J-LIS、NJSSなどのサマリ | 毎日 | API / スクレイピング |
| **議会議事録** | 「Web会議」「オンライン相談」等の発言 | 随時 | 議事録検索システム連携 / PDF解析 |
| **Google/Bing検索** | 自治体ドメイン(`site:lg.jp`)指定のニュース検索 | 毎日 | Custom Search API |

## 3. 「精度の高い情報だけ選別」するフィルタリング (The Filter)
集めた膨大なデータ（ノイズ）から、有望なシグナルだけを抽出・構造化します。ここがAIの主戦場です。

### フェーズ1: キーワードによる粗選別
- `["Zoom", "Webex", "Web会議", "オンライン窓口", "DX", "補正予算"]` などのキーワードが含まれないドキュメントは即破棄。

### フェーズ2: LLMによる意味理解とスコアリング
残ったテキストデータに対し、ローカルLLM（Ollama）またはAPI（Gemini/GPT）を用いて以下の判定を行います。

- **文脈判定**: 単に「Zoom」という単語があるだけか？それとも「Zoomのライセンス更新」について話しているか？
- **確度スコアリング (1-100)**:
  - 「検討を開始する」→ Score 30
  - 「予算を計上した」→ Score 80
  - 「公募を開始する」→ Score 100

### フェーズ3: 構造化データへの変換（特徴量化）
非構造化テキスト（ニュース、PDF）を、予測モデルに入れられる形（テーブルデータ）に変換します。

- `last_zoom_mention_date`: 最後にZoomと言及された日（鮮度）
- `budget_allocation_flag`: 予算関連文書での言及有無（1/0）
- `positive_sentiment_score`: 文脈が肯定的か否定的か

## 4. 学習データの考え方
- **Train Data**: 過去（直近3年）に実際にZoomを導入した、または問い合わせがあった自治体の「その当時のWeb上のシグナル」を再現して教師データとします。
- **検証**: 常に「最新の」データに対して予測を行い、営業担当者のフィードバック（「この自治体は確かに検討中だった」等）を即座にモデルに反映します（Human-in-the-loop）。
