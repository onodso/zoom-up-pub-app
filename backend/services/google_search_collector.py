"""
Google Custom Search API Integration
è‡ªæ²»ä½“ã®DXé–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹ã€Zoomå°å…¥äº‹ä¾‹ãªã©ã‚’å®Ÿãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦åé›†

å¿…è¦ãªç’°å¢ƒå¤‰æ•°:
- GOOGLE_API_KEY: Google Cloud Consoleã®APIã‚­ãƒ¼
- GOOGLE_CSE_ID: Programmable Search Engineã®ID
"""

import httpx
import os
from typing import List, Dict, Optional
import psycopg2
from psycopg2.extras import RealDictCursor
import json
from datetime import datetime


class GoogleNewsCollector:
    """Google Custom Search APIã§ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†"""

    def __init__(self):
        self.api_key = os.getenv("GOOGLE_API_KEY")
        self.cse_id = os.getenv("GOOGLE_CSE_ID")

        if not self.api_key or not self.cse_id:
            raise ValueError(
                "Missing credentials. Set GOOGLE_API_KEY and GOOGLE_CSE_ID in .env"
            )

        self.api_url = "https://www.googleapis.com/customsearch/v1"
        self.client = httpx.Client(timeout=30.0)

    def search(self, query: str, num_results: int = 10) -> List[Dict]:
        """
        Google Custom Searchã§æ¤œç´¢

        Args:
            query: æ¤œç´¢ã‚¯ã‚¨ãƒªï¼ˆä¾‹: "ç¦å²¡å¸‚ DXæ¨é€²"ï¼‰
            num_results: å–å¾—ä»¶æ•°ï¼ˆæœ€å¤§10ï¼‰

        Returns:
            æ¤œç´¢çµæœãƒªã‚¹ãƒˆ [
                {
                    'title': 'ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚¿ã‚¤ãƒˆãƒ«',
                    'link': 'https://...',
                    'snippet': 'è¦ç´„',
                    'date': '2024-01-15'
                },
                ...
            ]
        """
        try:
            params = {
                'key': self.api_key,
                'cx': self.cse_id,
                'q': query,
                'num': min(num_results, 10),
                'lr': 'lang_ja',  # æ—¥æœ¬èªã®ã¿
                'dateRestrict': 'y1',  # éå»1å¹´ä»¥å†…
            }

            response = self.client.get(self.api_url, params=params)
            response.raise_for_status()

            data = response.json()

            results = []
            for item in data.get('items', []):
                results.append({
                    'title': item.get('title', ''),
                    'link': item.get('link', ''),
                    'snippet': item.get('snippet', ''),
                    'source': item.get('displayLink', ''),
                    'date': self._extract_date(item)
                })

            return results

        except httpx.HTTPStatusError as e:
            print(f"âŒ API Error: {e.response.status_code} - {e.response.text}")
            return []
        except Exception as e:
            print(f"âŒ Search error: {e}")
            return []

    def _extract_date(self, item: Dict) -> Optional[str]:
        """æ¤œç´¢çµæœã‹ã‚‰æ—¥ä»˜ã‚’æŠ½å‡º"""
        # Googleæ¤œç´¢çµæœã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æ—¥ä»˜ã‚’å–å¾—
        if 'pagemap' in item and 'metatags' in item['pagemap']:
            metatags = item['pagemap']['metatags'][0]
            for date_field in ['article:published_time', 'datePublished', 'date']:
                if date_field in metatags:
                    return metatags[date_field][:10]  # YYYY-MM-DDå½¢å¼

        return None

    def search_dx_news(self, city_name: str) -> List[Dict]:
        """DXé–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’æ¤œç´¢"""
        queries = [
            f"{city_name} DXæ¨é€²",
            f"{city_name} ãƒ‡ã‚¸ã‚¿ãƒ«åŒ–",
            f"{city_name} ã‚¹ãƒãƒ¼ãƒˆã‚·ãƒ†ã‚£",
        ]

        all_results = []
        for query in queries:
            results = self.search(query, num_results=5)
            all_results.extend(results)

        # é‡è¤‡å‰Šé™¤
        unique_results = {r['link']: r for r in all_results}.values()
        return list(unique_results)

    def search_zoom_deployments(self, city_name: str) -> List[Dict]:
        """Zoomå°å…¥äº‹ä¾‹ã‚’æ¤œç´¢"""
        query = f"{city_name} Zoom å°å…¥"
        return self.search(query, num_results=10)

    def search_kasuhara_news(self, city_name: str) -> List[Dict]:
        """ã‚«ã‚¹ãƒãƒ©é–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’æ¤œç´¢"""
        query = f"{city_name} ã‚«ã‚¹ãƒãƒ© OR ã‚¯ãƒ¬ãƒ¼ãƒ  OR è‹¦æƒ…"
        return self.search(query, num_results=5)

    def close(self):
        self.client.close()


class NewsDataUpdater:
    """æ¤œç´¢çµæœã‚’ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜"""

    def __init__(self):
        self.conn = psycopg2.connect(
            host=os.getenv("POSTGRES_HOST", "localhost"),
            port=int(os.getenv("POSTGRES_PORT", "5432")),
            user=os.getenv("POSTGRES_USER", "zoom_admin"),
            password=os.getenv("POSTGRES_PASSWORD", "password"),
            dbname=os.getenv("POSTGRES_DB", "zoom_dx_db")
        )
        self.cur = self.conn.cursor(cursor_factory=RealDictCursor)
        self.create_table()

    def create_table(self):
        """ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆ"""
        self.cur.execute("""
            CREATE TABLE IF NOT EXISTS municipality_news (
                id SERIAL PRIMARY KEY,
                city_code VARCHAR(6),
                category VARCHAR(50),  -- dx, zoom, kasuhara
                title TEXT,
                url TEXT UNIQUE,
                snippet TEXT,
                source VARCHAR(200),
                published_date DATE,
                collected_at TIMESTAMP DEFAULT NOW(),
                FOREIGN KEY (city_code) REFERENCES municipalities(city_code)
            );

            CREATE INDEX IF NOT EXISTS idx_news_city_category
            ON municipality_news(city_code, category);
        """)
        self.conn.commit()

    def save_news(self, city_code: str, category: str, news_list: List[Dict]):
        """ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’ä¿å­˜"""
        saved_count = 0

        for news in news_list:
            try:
                self.cur.execute("""
                    INSERT INTO municipality_news
                        (city_code, category, title, url, snippet, source, published_date)
                    VALUES (%s, %s, %s, %s, %s, %s, %s)
                    ON CONFLICT (url) DO NOTHING;
                """, (
                    city_code,
                    category,
                    news['title'],
                    news['link'],
                    news['snippet'],
                    news['source'],
                    news['date']
                ))

                if self.cur.rowcount > 0:
                    saved_count += 1

            except Exception as e:
                print(f"âš ï¸  Failed to save: {e}")

        self.conn.commit()
        return saved_count

    def get_news_summary(self, city_code: str) -> Dict:
        """è‡ªæ²»ä½“ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚µãƒãƒªãƒ¼ã‚’å–å¾—"""
        self.cur.execute("""
            SELECT category, COUNT(*) as count
            FROM municipality_news
            WHERE city_code = %s
            GROUP BY category;
        """, (city_code,))

        summary = {row['category']: row['count'] for row in self.cur.fetchall()}
        return summary

    def close(self):
        self.cur.close()
        self.conn.close()


def collect_news_for_city(city_code: str, city_name: str):
    """
    1ã¤ã®è‡ªæ²»ä½“ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’åé›†

    Args:
        city_code: è‡ªæ²»ä½“ã‚³ãƒ¼ãƒ‰
        city_name: è‡ªæ²»ä½“å
    """
    print(f"\n{'='*80}")
    print(f"Collecting news for: {city_name} ({city_code})")
    print(f"{'='*80}")

    collector = GoogleNewsCollector()
    updater = NewsDataUpdater()

    try:
        # 1. DXé–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹
        print(f"\nğŸ” Searching DX news...")
        dx_news = collector.search_dx_news(city_name)
        saved = updater.save_news(city_code, 'dx', dx_news)
        print(f"âœ… Found {len(dx_news)} articles, saved {saved} new ones")

        if dx_news[:3]:
            print(f"\nğŸ“° Top 3 DX news:")
            for idx, news in enumerate(dx_news[:3], 1):
                print(f"   {idx}. {news['title'][:60]}...")
                print(f"      {news['link']}")

        # 2. Zoomå°å…¥äº‹ä¾‹
        print(f"\nğŸ” Searching Zoom deployments...")
        zoom_news = collector.search_zoom_deployments(city_name)
        saved = updater.save_news(city_code, 'zoom', zoom_news)
        print(f"âœ… Found {len(zoom_news)} articles, saved {saved} new ones")

        # 3. ã‚«ã‚¹ãƒãƒ©ãƒ‹ãƒ¥ãƒ¼ã‚¹
        print(f"\nğŸ” Searching kasuhara news...")
        kasuhara_news = collector.search_kasuhara_news(city_name)
        saved = updater.save_news(city_code, 'kasuhara', kasuhara_news)
        print(f"âœ… Found {len(kasuhara_news)} articles, saved {saved} new ones")

        # ã‚µãƒãƒªãƒ¼è¡¨ç¤º
        summary = updater.get_news_summary(city_code)
        print(f"\nğŸ“Š Total news in database:")
        print(f"   DX: {summary.get('dx', 0)}")
        print(f"   Zoom: {summary.get('zoom', 0)}")
        print(f"   Kasuhara: {summary.get('kasuhara', 0)}")

        print(f"\n{'='*80}\n")

    except Exception as e:
        print(f"âŒ Error: {e}")
        import traceback
        traceback.print_exc()

    finally:
        collector.close()
        updater.close()


if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆ: ç¦å²¡å¸‚ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†
    print("ğŸš€ Google Custom Search API - News Collection Test")
    print("=" * 80)

    # èªè¨¼æƒ…å ±ãƒã‚§ãƒƒã‚¯
    api_key = os.getenv("GOOGLE_API_KEY")
    cse_id = os.getenv("GOOGLE_CSE_ID")

    if not api_key or not cse_id:
        print("âŒ Missing credentials!")
        print("\nPlease add to .env:")
        print("  GOOGLE_API_KEY=your_api_key")
        print("  GOOGLE_CSE_ID=your_search_engine_id")
        print("\nGet credentials from:")
        print("  API Key: https://console.cloud.google.com/apis/credentials")
        print("  CSE ID:  https://programmablesearchengine.google.com/")
    else:
        print(f"âœ… API Key: {api_key[:10]}...")
        print(f"âœ… CSE ID:  {cse_id[:20]}...")
        print()

        # ç¦å²¡å¸‚ã§ãƒ†ã‚¹ãƒˆ
        collect_news_for_city('401307', 'ç¦å²¡å¸‚')
