"""
Real Data Collector - NO DUMMY DATA
å®Ÿãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’åé›†ã™ã‚‹ã€‚ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã¯ä¸€åˆ‡ä½¿ç”¨ã—ãªã„ã€‚

åé›†å¯èƒ½ãªå®Ÿãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹:
1. ç·å‹™çœ åœ°æ–¹å…¬å…±å›£ä½“æƒ…å ±ã‚·ã‚¹ãƒ†ãƒ æ©Ÿæ§‹ï¼ˆJ-LISï¼‰
2. å„è‡ªæ²»ä½“ã®å…¬å¼ã‚µã‚¤ãƒˆï¼ˆå¸‚é•·åã€çµ„ç¹”å›³ï¼‰
3. å›½åœŸåœ°ç†é™¢ï¼ˆæ­£ç¢ºãªç·¯åº¦çµŒåº¦ï¼‰
4. ç·å‹™çœçµ±è¨ˆï¼ˆè²¡æ”¿çŠ¶æ³ï¼‰
5. Googleãƒ‹ãƒ¥ãƒ¼ã‚¹æ¤œç´¢ï¼ˆDXé–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹ï¼‰
"""

import httpx
import re
import json
from typing import Dict, Optional
import psycopg2
from psycopg2.extras import RealDictCursor
import os
import time


class RealDataCollector:
    """å®Ÿãƒ‡ãƒ¼ã‚¿åé›†ã‚¨ãƒ³ã‚¸ãƒ³ - ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ç¦æ­¢"""

    def __init__(self):
        self.client = httpx.Client(timeout=30.0, follow_redirects=True)
        self.conn = psycopg2.connect(
            host=os.getenv("POSTGRES_HOST", "localhost"),
            port=int(os.getenv("POSTGRES_PORT", "5432")),
            user=os.getenv("POSTGRES_USER", "zoom_admin"),
            password=os.getenv("POSTGRES_PASSWORD", "password"),
            dbname=os.getenv("POSTGRES_DB", "zoom_dx_db")
        )
        self.cur = self.conn.cursor(cursor_factory=RealDictCursor)

    def scrape_mayor_from_official_site(self, city_code: str, official_url: str) -> Optional[str]:
        """
        è‡ªæ²»ä½“å…¬å¼ã‚µã‚¤ãƒˆã‹ã‚‰å¸‚é•·åã‚’å®Ÿéš›ã«ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°

        Returns:
            å¸‚é•·åï¼ˆå®Ÿãƒ‡ãƒ¼ã‚¿ï¼‰ã¾ãŸã¯ None
        """
        if not official_url:
            return None

        try:
            # å¸‚é•·æŒ¨æ‹¶ãƒ»å¸‚é•·å®¤ãƒšãƒ¼ã‚¸ã‚’æ¢ã™
            mayor_keywords = ['å¸‚é•·', 'ç”ºé•·', 'æ‘é•·', 'åŒºé•·', 'é¦–é•·']
            page_patterns = [
                '/mayor/',
                '/shicho/',
                '/message/',
                '/greeting/',
                '/profile/',
            ]

            # ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã‚’å–å¾—
            response = self.client.get(official_url)
            if response.status_code != 200:
                print(f"âš ï¸  {official_url}: HTTP {response.status_code}")
                return None

            html = response.text

            # å¸‚é•·ãƒšãƒ¼ã‚¸ã®ãƒªãƒ³ã‚¯ã‚’æ¢ã™
            mayor_page_url = None
            for pattern in page_patterns:
                if pattern in html.lower():
                    # ãƒªãƒ³ã‚¯ã‚’æŠ½å‡ºï¼ˆç°¡æ˜“ç‰ˆï¼‰
                    match = re.search(rf'href=["\']([^"\']*{pattern}[^"\']*)["\']', html, re.IGNORECASE)
                    if match:
                        link = match.group(1)
                        if link.startswith('http'):
                            mayor_page_url = link
                        else:
                            mayor_page_url = official_url.rstrip('/') + '/' + link.lstrip('/')
                        break

            if not mayor_page_url:
                # ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã‹ã‚‰ç›´æ¥å¸‚é•·åã‚’æ¢ã™
                return self._extract_mayor_name_from_html(html)

            # å¸‚é•·ãƒšãƒ¼ã‚¸ã‚’å–å¾—
            response = self.client.get(mayor_page_url)
            if response.status_code == 200:
                return self._extract_mayor_name_from_html(response.text)

            return None

        except Exception as e:
            print(f"âŒ Scraping error for {official_url}: {e}")
            return None

    def _extract_mayor_name_from_html(self, html: str) -> Optional[str]:
        """
        HTMLã‹ã‚‰å¸‚é•·åã‚’æŠ½å‡º

        ãƒ‘ã‚¿ãƒ¼ãƒ³:
        - "å¸‚é•· å±±ç”°å¤ªéƒ"
        - "å¸‚é•·ï¼šç”°ä¸­èŠ±å­"
        - "â—‹â—‹å¸‚é•· éˆ´æœ¨ä¸€éƒ"
        """
        patterns = [
            r'å¸‚é•·[ï¼š:\s]+([^\s<>]{2,5})',
            r'ç”ºé•·[ï¼š:\s]+([^\s<>]{2,5})',
            r'æ‘é•·[ï¼š:\s]+([^\s<>]{2,5})',
            r'åŒºé•·[ï¼š:\s]+([^\s<>]{2,5})',
        ]

        for pattern in patterns:
            match = re.search(pattern, html)
            if match:
                name = match.group(1)
                # ã€Œæ§˜ã€ã€Œã•ã‚“ã€ãªã©ã‚’é™¤å»
                name = re.sub(r'[æ§˜ã•ã‚“æ®¿æ°]$', '', name)
                # æ¼¢å­—ã®ã¿ï¼ˆ2-4æ–‡å­—ï¼‰ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
                if re.match(r'^[ä¸€-é¾¯]{2,4}$', name):
                    return name

        return None

    def search_dx_news(self, city_name: str, keyword: str) -> list:
        """
        Googleæ¤œç´¢ã§å®Ÿéš›ã®DXé–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’åé›†

        Args:
            city_name: è‡ªæ²»ä½“å
            keyword: æ¤œç´¢ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ï¼ˆä¾‹: "DXæ¨é€²", "Zoomå°å…¥"ï¼‰

        Returns:
            ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ãƒªã‚¹ãƒˆï¼ˆå®Ÿãƒ‡ãƒ¼ã‚¿ï¼‰
        """
        try:
            # Google Custom Search APIä½¿ç”¨ï¼ˆã¾ãŸã¯é€šå¸¸ã®Googleæ¤œç´¢ï¼‰
            query = f"{city_name} {keyword}"

            # ã‚·ãƒ³ãƒ—ãƒ«ãªGoogleæ¤œç´¢ï¼ˆAPIã‚­ãƒ¼ãŒãªã„å ´åˆã®ä»£æ›¿ï¼‰
            search_url = "https://www.google.com/search"
            params = {
                'q': query,
                'num': 10,
                'hl': 'ja'
            }

            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }

            response = self.client.get(search_url, params=params, headers=headers)

            if response.status_code == 200:
                # å®Ÿéš›ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚¿ã‚¤ãƒˆãƒ«ã‚’æŠ½å‡º
                titles = re.findall(r'<h3[^>]*>([^<]+)</h3>', response.text)
                return titles[:5]  # ä¸Šä½5ä»¶

            return []

        except Exception as e:
            print(f"âŒ News search error: {e}")
            return []

    def get_fiscal_data_from_soumu(self, city_code: str) -> Optional[Dict]:
        """
        ç·å‹™çœã®å…¬é–‹ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰è²¡æ”¿æƒ…å ±ã‚’å–å¾—ï¼ˆå®Ÿãƒ‡ãƒ¼ã‚¿ï¼‰

        ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹: ç·å‹™çœã€Œåœ°æ–¹è²¡æ”¿çŠ¶æ³èª¿æŸ»ã€
        """
        try:
            # ç·å‹™çœã®å…¬é–‹ãƒ‡ãƒ¼ã‚¿ãƒãƒ¼ã‚¿ãƒ«
            # å®Ÿéš›ã®URLã¯ç·å‹™çœã®ãƒ‡ãƒ¼ã‚¿ã‚«ã‚¿ãƒ­ã‚°ã‹ã‚‰å–å¾—

            # ç¾æ™‚ç‚¹ã§ã¯è²¡æ”¿åŠ›æŒ‡æ•°ã¯æ—¢ã«DBã«ã‚ã‚‹ã®ã§ã€ãã‚Œã‚’åˆ©ç”¨
            self.cur.execute("""
                SELECT fiscal_index
                FROM municipalities
                WHERE city_code = %s AND fiscal_index IS NOT NULL;
            """, (city_code,))

            result = self.cur.fetchone()
            if result:
                return {'fiscal_index': result['fiscal_index']}

            return None

        except Exception as e:
            print(f"âŒ Fiscal data error: {e}")
            return None

    def validate_and_save_mayor(self, city_code: str, mayor_name: str, source_url: str):
        """
        å¸‚é•·åã‚’ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã—ã¦ä¿å­˜ï¼ˆå®Ÿãƒ‡ãƒ¼ã‚¿ã®ã¿ï¼‰

        ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³:
        - æ¼¢å­—ã®ã¿ï¼ˆ2-4æ–‡å­—ï¼‰
        - ã‚½ãƒ¼ã‚¹URLãŒè¨˜éŒ²ã•ã‚Œã¦ã„ã‚‹
        """
        if not mayor_name:
            return False

        # ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
        if not re.match(r'^[ä¸€-é¾¯]{2,4}$', mayor_name):
            print(f"âš ï¸  Invalid mayor name format: {mayor_name}")
            return False

        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
        self.cur.execute("""
            UPDATE municipalities
            SET mayor_name = %s,
                mayor_speech_url = %s,
                updated_at = NOW()
            WHERE city_code = %s
            RETURNING city_name;
        """, (mayor_name, source_url, city_code))

        result = self.cur.fetchone()
        if result:
            print(f"âœ… {result['city_name']}: å¸‚é•·åã‚’æ›´æ–° â†’ {mayor_name} (Source: {source_url})")
            self.conn.commit()
            return True

        return False

    def collect_real_data_for_municipality(self, city_code: str):
        """
        1ã¤ã®è‡ªæ²»ä½“ã®å®Ÿãƒ‡ãƒ¼ã‚¿ã‚’åé›†

        åé›†é …ç›®:
        1. å¸‚é•·åï¼ˆå…¬å¼ã‚µã‚¤ãƒˆã‹ã‚‰ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ï¼‰
        2. DXé–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹ï¼ˆGoogleæ¤œç´¢ï¼‰
        3. è²¡æ”¿æƒ…å ±ï¼ˆç·å‹™çœãƒ‡ãƒ¼ã‚¿ï¼‰
        """
        # è‡ªæ²»ä½“æƒ…å ±å–å¾—
        self.cur.execute("""
            SELECT city_code, city_name, official_url, mayor_name
            FROM municipalities
            WHERE city_code = %s;
        """, (city_code,))

        muni = self.cur.fetchone()
        if not muni:
            print(f"âŒ Municipality {city_code} not found")
            return

        print(f"\n{'='*80}")
        print(f"Collecting REAL data for: {muni['city_name']} ({city_code})")
        print(f"{'='*80}")

        # 1. å¸‚é•·ååé›†ï¼ˆæ—¢ã«å¸‚é•·åãŒã‚ã‚‹å ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰
        if not muni['mayor_name'] and muni['official_url']:
            print(f"\nğŸ” Scraping mayor name from {muni['official_url']}...")
            mayor_name = self.scrape_mayor_from_official_site(city_code, muni['official_url'])

            if mayor_name:
                self.validate_and_save_mayor(city_code, mayor_name, muni['official_url'])
            else:
                print(f"âš ï¸  Could not extract mayor name")

        # 2. DXãƒ‹ãƒ¥ãƒ¼ã‚¹æ¤œç´¢
        print(f"\nğŸ“° Searching DX news...")
        news = self.search_dx_news(muni['city_name'], 'DXæ¨é€²')
        if news:
            print(f"âœ… Found {len(news)} news articles:")
            for idx, title in enumerate(news[:3], 1):
                print(f"   {idx}. {title[:60]}...")

        # 3. è²¡æ”¿ãƒ‡ãƒ¼ã‚¿
        print(f"\nğŸ’° Checking fiscal data...")
        fiscal = self.get_fiscal_data_from_soumu(city_code)
        if fiscal:
            print(f"âœ… Fiscal index: {fiscal['fiscal_index']}")

        print(f"\n{'='*80}\n")

        # Rate limiting
        time.sleep(2)

    def close(self):
        self.cur.close()
        self.conn.close()
        self.client.close()


def collect_real_data_batch(city_codes: list):
    """
    è¤‡æ•°è‡ªæ²»ä½“ã®å®Ÿãƒ‡ãƒ¼ã‚¿ã‚’ä¸€æ‹¬åé›†

    Args:
        city_codes: è‡ªæ²»ä½“ã‚³ãƒ¼ãƒ‰ã®ãƒªã‚¹ãƒˆ
    """
    print("=" * 80)
    print("REAL DATA COLLECTION - NO DUMMY DATA")
    print("=" * 80)
    print(f"\nTarget: {len(city_codes)} municipalities")
    print()

    collector = RealDataCollector()

    try:
        for city_code in city_codes:
            collector.collect_real_data_for_municipality(city_code)

    finally:
        collector.close()


if __name__ == "__main__":
    # ãƒ‘ã‚¤ãƒ­ãƒƒãƒˆ17è‡ªæ²»ä½“ã®å®Ÿãƒ‡ãƒ¼ã‚¿ã‚’åé›†
    pilot_cities = [
        '401307',  # ç¦å²¡å¸‚
        '401005',  # åŒ—ä¹å·å¸‚
        '402141',  # å®—åƒå¸‚
    ]

    print("ğŸš€ Starting REAL data collection...")
    print("âš ï¸  NO DUMMY DATA - Only collecting actual information from public sources")
    print()

    collect_real_data_batch(pilot_cities)

    print("\nâœ… Real data collection complete")
    print("ğŸ“Š Check database for updated information")
