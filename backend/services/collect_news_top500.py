"""
Collect News for Top Municipalities
äººå£ä¸Šä½ã®è‡ªæ²»ä½“ã‚’å¯¾è±¡ã«ã€DXãƒ»Zoomãƒ»ã‚«ã‚¹ãƒãƒ©é–¢é€£ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’åé›†ã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ

APIåˆ¶é™ï¼ˆ1æ—¥100å›ç„¡æ–™ã‚’æƒ³å®šï¼‰ã‚’è€ƒæ…®ã—ã€
ã¾ãšã¯ Top 10 è‡ªæ²»ä½“ Ã— 3ã‚«ãƒ†ã‚´ãƒª = 30ã‚¯ã‚¨ãƒª ã§å®Ÿè¡Œã™ã‚‹ã€‚
"""

import os
import time
import psycopg2
from psycopg2.extras import RealDictCursor
from google_search_collector import GoogleNewsCollector, NewsDataUpdater

# APIãƒ¬ãƒ¼ãƒˆãƒªãƒŸãƒƒãƒˆå¯¾ç­–
# ç„¡æ–™æ : 1,500ãƒªã‚¯ã‚¨ã‚¹ãƒˆ/æ—¥
# åˆ¥ã‚¢ãƒ—ãƒªä½¿ç”¨: 50-150å›/æ—¥ï¼ˆå¹³å‡100å›ï¼‰
# æœ¬ã‚¢ãƒ—ãƒªç›®æ¨™: 1,020å›/æ—¥ (340è‡ªæ²»ä½“ Ã— 3ã‚«ãƒ†ã‚´ãƒª)
# åˆè¨ˆ: 1,120-1,170å›/æ—¥ (ç„¡æ–™æ ã®75%ç¨‹åº¦)
# å…¨2,406è‡ªæ²»ä½“ã‚’8æ—¥é–“ã§å®Œäº†
BATCH_SIZE = 340
SLEEP_BETWEEN_QUERIES = 1.0

def get_next_batch_municipalities(limit: int):
    """
    ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†å¯¾è±¡ã®è‡ªæ²»ä½“ã‚’å–å¾—ï¼ˆãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰
    
    1. äººå£ä¸Šä½500è‡ªæ²»ä½“ã‚’å¯¾è±¡ã¨ã™ã‚‹
    2. ãã®ä¸­ã§ã€ãƒ‹ãƒ¥ãƒ¼ã‚¹æœ€çµ‚åé›†æ—¥æ™‚(collected_at)ãŒå¤ã„é †ã€
       ã¾ãŸã¯æœªåé›†(NULL)ã®è‡ªæ²»ä½“ã‚’å„ªå…ˆã—ã¦å–å¾—ã™ã‚‹
    """
    conn = psycopg2.connect(
        host=os.getenv("POSTGRES_HOST", "localhost"),
        port=int(os.getenv("POSTGRES_PORT", "5432")),
        user=os.getenv("POSTGRES_USER", "zoom_admin"),
        password=os.getenv("POSTGRES_PASSWORD", "password"),
        dbname=os.getenv("POSTGRES_DB", "zoom_dx_db")
    )
    cur = conn.cursor(cursor_factory=RealDictCursor)
    
    try:
        # è¤‡é›‘ãªã‚¯ã‚¨ãƒªã«ãªã‚‹ãŸã‚ã€CTEã‚’ä½¿ç”¨
        cur.execute("""
            WITH top_500 AS (
                SELECT city_code, city_name, prefecture, population
                FROM municipalities
                WHERE population IS NOT NULL
                ORDER BY population DESC
                LIMIT 500
            ),
            latest_collection AS (
                SELECT city_code, MAX(collected_at) as last_collected
                FROM municipality_news
                GROUP BY city_code
            )
            SELECT 
                t.city_code, 
                t.city_name, 
                t.prefecture, 
                t.population,
                l.last_collected
            FROM top_500 t
            LEFT JOIN latest_collection l ON t.city_code = l.city_code
            ORDER BY l.last_collected ASC NULLS FIRST
            LIMIT %s;
        """, (limit,))
        return cur.fetchall()
    finally:
        cur.close()
        conn.close()

def main():
    print(f"ğŸš€ Starting Daily News Collection (Batch Size: {BATCH_SIZE})")
    print("=" * 60)
    
    # APIã‚­ãƒ¼ç¢ºèª
    if not os.getenv("GOOGLE_API_KEY") or not os.getenv("GOOGLE_CSE_ID"):
        print("âŒ Error: GOOGLE_API_KEY and GOOGLE_CSE_ID must be set.")
        return

    try:
        municipalities = get_next_batch_municipalities(BATCH_SIZE)
        print(f"ğŸ“‹ Target Municipalities: {len(municipalities)}")
        for m in municipalities:
            last_date = m.get('last_collected')
            status = f"Last processed: {last_date}" if last_date else "Never processed"
            print(f"   - {m['city_name']} ({m['prefecture']}): {status}")
        print("-" * 60)
        
        collector = GoogleNewsCollector()
        updater = NewsDataUpdater()
        
        total_queries = 0
        total_saved = 0
        
        for i, muni in enumerate(municipalities, 1):
            city_code = muni['city_code']
            city_name = muni['city_name']
            
            print(f"\n[{i}/{len(municipalities)}] Processing: {city_name}")
            
            try:
                # 1. DX News
                print("   ğŸ” DX News...", end="", flush=True)
                dx_news = collector.search_dx_news(city_name)
                saved_dx = updater.save_news(city_code, 'dx', dx_news)
                print(f" Found {len(dx_news)}, Saved {saved_dx}")
                time.sleep(SLEEP_BETWEEN_QUERIES)
                total_queries += 1 # search_dx_newså†…ã§è¤‡æ•°ã‚¯ã‚¨ãƒªæŠ•ã’ã¦ã„ã‚‹å ´åˆã¯ä¿®æ­£ãŒå¿…è¦ã ãŒã€ä¸€æ—¦ç°¡æ˜“ã‚«ã‚¦ãƒ³ãƒˆ
                
                # 2. Zoom News
                print("   ğŸ” Zoom News...", end="", flush=True)
                zoom_news = collector.search_zoom_deployments(city_name)
                saved_zoom = updater.save_news(city_code, 'zoom', zoom_news)
                print(f" Found {len(zoom_news)}, Saved {saved_zoom}")
                time.sleep(SLEEP_BETWEEN_QUERIES)
                total_queries += 1
                
                # 3. Kasuhara News
                print("   ğŸ” Kasuhara News...", end="", flush=True)
                kasuhara_news = collector.search_kasuhara_news(city_name)
                saved_kasuhara = updater.save_news(city_code, 'kasuhara', kasuhara_news)
                print(f" Found {len(kasuhara_news)}, Saved {saved_kasuhara}")
                time.sleep(SLEEP_BETWEEN_QUERIES)
                total_queries += 1
                
                total_saved += (saved_dx + saved_zoom + saved_kasuhara)
                
            except Exception as e:
                print(f"\nâŒ Error processing {city_name}: {e}")
                # ã‚¨ãƒ©ãƒ¼ãŒå‡ºã¦ã‚‚ç¶šè¡Œ
        
        print("\n" + "=" * 60)
        print("âœ… Collection Complete!")
        print(f"   Total Municipalities: {len(municipalities)}")
        print(f"   Total Saved Articles: {total_saved}")
        print(f"   Est. Queries Used:    ~{total_queries} (plus internal multi-queries)")
        print("=" * 60)
        
    except Exception as e:
        print(f"\nâŒ Fatal Error: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
